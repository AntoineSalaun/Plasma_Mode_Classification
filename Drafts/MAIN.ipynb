{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa66c58",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf47afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# data packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# torch libraries\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "# import dlc_practical_prologue as prologue\n",
    "# train_input, train_target, test_input, test_target = \\\n",
    "#     prologue.load_data(one_hot_labels = True, normalize = True, flatten = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba36397",
   "metadata": {},
   "source": [
    "# SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "162f120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data splitting\n",
    "def split_data(x, y, ratio=0.90, seed=0):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    dataset_size = x.shape[0]\n",
    "    indices = np.random.permutation(dataset_size)\n",
    "    threshold  = int(ratio * dataset_size)\n",
    "    index_train = indices[:threshold]\n",
    "    index_test = indices[threshold:]\n",
    "    # create split\n",
    "    x_training = x[index_train]\n",
    "    x_test = x[index_test]\n",
    "    y_training = y[index_train]\n",
    "    y_test = y[index_test]\n",
    "    return x_training, x_test, y_training, y_test\n",
    "\n",
    "#### data importing \n",
    "parquet_file = 'TCV_LHD_db4ML.parquet.part'\n",
    "df = pd.read_parquet(parquet_file, engine ='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e7e9302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .cat accessor with a 'category' dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kc/5wgtb93s7y9d5mzpdd41qclw0000gn/T/ipykernel_12926/2423039089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#remove Nan values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#reset indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLDH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLDH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ip<Ip_MIN'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#remove Ip<Ip_MIN category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2601\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   2608\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2610\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .cat accessor with a 'category' dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_delegate_property_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .cat accessor with a 'category' dtype"
     ]
    }
   ],
   "source": [
    "#### removing spurious data\n",
    "mask = df['LDH'] == 'Ip<Ip_MIN'\n",
    "df_filter = df.drop(index = df[mask].index) #remove Ip<Ip_MIN values \n",
    "\n",
    "df_filter = df_filter.dropna() #remove Nan values\n",
    "df_filter = df_filter.reset_index(drop=True) #reset indexing\n",
    "df_filter.LDH = df_filter.LDH.cat.remove_categories('Ip<Ip_MIN') #remove Ip<Ip_MIN category\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "discard_data = len(df.index) - len(df_filter.index) # number of data point that do not contain useful information\n",
    "print('number of useless data points: ', discard_data)\n",
    "print('size of filtered data set: ', len(df_filter.index))\n",
    "print('size of original data set: ', len(df.index))\n",
    "print(len(df_filter.index) + discard_data - len(df.index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter[df_filter['pulse'] == 1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "def create_freq_PD(data_frame):\n",
    "    fs = 1e4\n",
    "    \n",
    "    frequency_extended = np.array([]) \n",
    "    \n",
    "    for i in range(1,1+int(max(df_filter['pulse'].values))):\n",
    "        #print(i)\n",
    "        f, t, Sxx = signal.spectrogram(df_filter[df_filter['pulse'] == i]['PD'], fs,nperseg = 512, noverlap = 64 )\n",
    "        #plt.pcolormesh(t, f, Sxx, shading='gouraud')\n",
    "        #plt.ylabel('Frequency [Hz]')\n",
    "        #plt.xlabel('Time [sec]')\n",
    "        #plt.show\n",
    "        avg = np.zeros((Sxx.shape[1]))\n",
    "        avg = np.mean(Sxx, axis=0)\n",
    "        t.shape\n",
    "        current = np.resize(avg,df_filter[df_filter['pulse'] == i]['time'].shape)\n",
    "    \n",
    "        \n",
    "        frequency_extended = np.concatenate((frequency_extended,current))\n",
    "        \n",
    "    \n",
    "    data_frame['PD_freq'] = frequency_extended\n",
    "\n",
    "create_freq_PD(df_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579d524",
   "metadata": {},
   "source": [
    "# Reshaping the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12ab207a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 7 but corresponding boolean dimension is 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kc/5wgtb93s7y9d5mzpdd41qclw0000gn/T/ipykernel_12926/2186536879.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mfeatures_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mmask_features_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mfeatures_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_exp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_features_exp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mx_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdf_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_exp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 7 but corresponding boolean dimension is 8"
     ]
    }
   ],
   "source": [
    "\n",
    "##########################################################\n",
    "# separation into experiments\n",
    "# contruction of labels to numerical values\n",
    "\n",
    "total_samples = 0\n",
    "counter = 0 \n",
    "window_size = 40\n",
    "num_features = 5\n",
    "num_pulses=int(max(df_filter['pulse'].values)) # tot number of different pulses == tot number of different experiments\n",
    "\n",
    "number_correct_samples = 127832\n",
    "all_samples  = np.zeros(127832 * window_size * num_features).reshape((-1, window_size, num_features))\n",
    "all_labels = np.zeros(127832 * 3).reshape((-1, 3))\n",
    "\n",
    "for k in range(num_pulses):    \n",
    "    #print('running experiment ', k+1 )\n",
    "    mask_experiment = df_filter.pulse == k + 1\n",
    "    df_experiment = df_filter[mask_experiment]\n",
    "    df_experiment = df_experiment.reset_index(drop=True)    \n",
    "    \n",
    "    # labels\n",
    "    maskl = df_experiment.LDH == 'L'\n",
    "    maskd = df_experiment.LDH == 'D'\n",
    "    maskh = df_experiment.LDH == 'H'\n",
    "    labels = np.vstack((maskl, maskd, maskh)).T + 0.0\n",
    "    \n",
    "    features_exp = df_experiment.keys().to_numpy()\n",
    "    mask_features_exp = np.array([False, True, True, True, True, False, False, True ])\n",
    "    features_exp = features_exp[mask_features_exp]\n",
    "    x_exp = np.array( df_experiment.loc[:, features_exp].values )    \n",
    "\n",
    "    # this number varies from one experiment to another  \n",
    "    num_samples = int( x_exp.shape[0]/window_size )\n",
    "    step = 0\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        all_samples[i + counter] = x_exp[ step : step + window_size, : ].reshape((-1, window_size, num_features)) \n",
    "        all_labels[i + counter] = labels[ step : step + window_size, : ].mean(axis = 0).reshape((-1, 1, 3))                \n",
    "        step += window_size\n",
    "        \n",
    "    counter +=num_samples\n",
    "    total_samples  += num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13c433a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 40, 5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def balance_subset(samples,labels):\n",
    "\n",
    "    number_of_L = np.sum(labels, axis=0)\n",
    "    scale = np.floor(min(number_of_L))\n",
    "\n",
    "    L_rows = np.where(labels[:,0]==1)\n",
    "    D_rows = np.where(labels[:,1]==1)\n",
    "    H_rows = np.where(labels[:,2]==1)\n",
    "    \n",
    "    minimum = min(len(L_rows[0]),len(D_rows[0]),len(H_rows[0]))\n",
    "    \n",
    "    sub_L = np.random.choice(L_rows[0] , minimum)\n",
    "    sub_D = np.random.choice(D_rows[0] , minimum)\n",
    "    sub_H = np.random.choice(H_rows[0] , minimum)\n",
    "    \n",
    "    rows=np.concatenate((sub_L, sub_D,sub_H), axis=None)\n",
    "    len(rows)/3\n",
    "    \n",
    "    balanced_labels = labels[rows]\n",
    "    balanced_samples = samples[rows]\n",
    "    \n",
    "    return balanced_samples, balanced_labels\n",
    "\n",
    "balanced_x, balanced_y = balance_subset(all_samples,all_labels)\n",
    "balanced_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a780d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create of train and test set\n",
    "validation_split = 0.30\n",
    "shuffle_dataset = True\n",
    "random_seed= 0\n",
    "dataset_size = all_samples.shape[0]\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "x_train = all_samples[train_indices]\n",
    "y_train = all_labels[train_indices]\n",
    "\n",
    "x_test = all_samples[test_indices]\n",
    "y_test = all_labels[test_indices]\n",
    "\n",
    "df_test = pd.DataFrame(x_test[:, 0, 0])\n",
    "\n",
    "# transform into a tensor\n",
    "x_train = torch.from_numpy(x_train).float().reshape(-1, 1, window_size, num_features)[: 9000 ]\n",
    "y_train = torch.from_numpy(y_train).float()[: 9000 ]\n",
    "x_test = torch.from_numpy(x_test).float().reshape(-1, 1, window_size, num_features)[: 9000 ]\n",
    "y_test = torch.from_numpy(y_test).float()[: 9000 ]  \n",
    "\n",
    "#print('train set shape: ', x_train.shape)\n",
    "#print('test set shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23c80317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    train or test data must be torch.Size([1000, 1, 28, 28])\\n    size explanation tensor ( [ # N_samples , # channels_input, height, width ] )                               \\n                       \\n    target or label tensor \\n    torch.Size([N_samples, # labels/classes])\\n                                   \\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### info regarding data shape for models to work \n",
    "'''\n",
    "    train or test data must be torch.Size([1000, 1, 28, 28])\n",
    "    size explanation tensor ( [ # N_samples , # channels_input, height, width ] )                               \n",
    "                       \n",
    "    target or label tensor \n",
    "    torch.Size([N_samples, # labels/classes])\n",
    "                                   \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f1997",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ac1ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=2, padding =1, padding_mode = 'replicate' ),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=2, padding =1, padding_mode = 'replicate' ),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=2, padding =1, padding_mode = 'replicate' ),            \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "                \n",
    "        self.fc1 = nn.Linear( 32 * 11 * 2 , 2 * nb_hidden)\n",
    "        self.fc2 = nn.Linear( 2 * nb_hidden , nb_hidden )\n",
    "        self.fc3 = nn.Linear( nb_hidden , nb_hidden )\n",
    "        self.fc4 = nn.Linear( nb_hidden, 3)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # ouput shape torch.Size([100, 16, 10, 2])\n",
    "        x = self.conv2(x) # ouput shape torch.Size([100, 16, 5, 1])\n",
    "        x = self.conv3(x) # ouput shape torch.Size([100, 32, 3, 1])        \n",
    "        x = x.view(-1, 32 * 11 * 2  )\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout( x )\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout( x )\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout( x )\n",
    "        x = self.fc4(x)         \n",
    "        #return x\n",
    "        return F.softmax(x, dim = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4c32361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size, nb_epochs = 10):\n",
    "    lr = 1e-8\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = lr)    \n",
    "\n",
    "    for e in tqdm(range(nb_epochs)):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):            \n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a574545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    eps = 0.2 # tolerance\n",
    "    nb_data_errors = 0    \n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))    \n",
    "        #predicted_classes , _ = output.max(dim  = 0)     \n",
    "        for k in range(mini_batch_size):\n",
    "            if torch.norm(data_target[b + k] - output[k])  > eps :\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "    return nb_data_errors            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8da3e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_nb_errors(model, data_input, target, mini_batch_size):\n",
    "#     nb_errors = 0\n",
    "#     for b in range(0, data_input.size(0), mini_batch_size):\n",
    "#         output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "#         _, predicted_classes = output.max(1)\n",
    "#         for k in range(mini_batch_size):\n",
    "#             if target[b + k, predicted_classes[k]] <= 0:\n",
    "#                 nb_errors = nb_errors + 1\n",
    "#     return nb_errors\n",
    "\n",
    "#######################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56d9bb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (275) to match target batch_size (100).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kc/5wgtb93s7y9d5mzpdd41qclw0000gn/T/ipykernel_12926/2220116899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training started'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kc/5wgtb93s7y9d5mzpdd41qclw0000gn/T/ipykernel_12926/2276238939.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_input, train_target, mini_batch_size, nb_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (275) to match target batch_size (100)."
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "nb_hidden = 10\n",
    "# fake data to test if model runs\n",
    "# torch.manual_seed(100)\n",
    "\n",
    "# to train in GPU if available \n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cuda')\n",
    "# print('working device ', device)\n",
    "# Net2.to(device)\n",
    "# x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "model = Net2(nb_hidden)\n",
    "model.train()\n",
    "print('training started')\n",
    "train_model(model, x_train, y_train, mini_batch_size)\n",
    "print('training finished')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b7bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e80835c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9000, 1, 40, 5])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 660\n",
    "model(x_test).shape\n",
    "\n",
    "x_test_2 = x_test.reshape(-1, 1, 20, 4)[: 63900 ]\n",
    "\n",
    "x_test.shape\n",
    "#df_X_test = pd.DataFrame(x_test.numpy())\n",
    "#x_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c1b6b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net2 0.00%% 0/9000\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    eps = 0.2 # tolerance\n",
    "    nb_data_errors = 0    \n",
    "    for b in range(0, data_input.size(0)- mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))    \n",
    "        \n",
    "        \n",
    "        # HERE WE SHOULD USE THE KAPPA FUNCTION\n",
    "        \n",
    "        #predicted_classes , _ = output.max(dim  = 0)     \n",
    "        #print(output)\n",
    "    return nb_data_errors \n",
    "\n",
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):    \n",
    "    nb_data_errors = 0    \n",
    "    with torch.no_grad():\n",
    "        for b in range(0, data_input.size(0), mini_batch_size):\n",
    "            output = model(data_input.narrow(0, b, mini_batch_size))    \n",
    "            _, predicted = torch.max(output, 1)    \n",
    "            for k in range(mini_batch_size):\n",
    "                if data_target[b + k].view(-1, 3).max(1)[1]  != predicted[k] :\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "    return nb_data_errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def probability_to_category(prediction):\n",
    "    prediction['prediction'] = prediction[['H','L','D']].idxmax(axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "nb_test_errors = compute_nb_errors(model, x_test, y_test, mini_batch_size)\n",
    "print('test error Net2 {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / x_train.size(0),\n",
    "                                                    nb_test_errors, x_test.size(0)))\n",
    "# test functions\n",
    "# x_test.narrow(0, 0, mini_batch_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2351f6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3483756826.py, line 95)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/kc/5wgtb93s7y9d5mzpdd41qclw0000gn/T/ipykernel_12926/3483756826.py\"\u001b[0;36m, line \u001b[0;32m95\u001b[0m\n\u001b[0;31m    for i in range(num_samples-):\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# data packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# torch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "# import dlc_practical_prologue as prologue\n",
    "# train_input, train_target, test_input, test_target = \\\n",
    "#     prologue.load_data(one_hot_labels = True, normalize = True, flatten = False)\n",
    "\n",
    "##########################################################\n",
    "### data splitting\n",
    "def split_data(x, y, ratio=0.90, seed=0):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    dataset_size = x.shape[0]\n",
    "    indices = np.random.permutation(dataset_size)\n",
    "    threshold  = int(ratio * dataset_size)\n",
    "    index_train = indices[:threshold]\n",
    "    index_test = indices[threshold:]\n",
    "    # create split\n",
    "    x_training = x[index_train]\n",
    "    x_test = x[index_test]\n",
    "    y_training = y[index_train]\n",
    "    y_test = y[index_test]\n",
    "    return x_training, x_test, y_training, y_test\n",
    "\n",
    "#### data importing \n",
    "parquet_file = 'TCV_LHD_db4ML.parquet.part'\n",
    "df = pd.read_parquet(parquet_file, engine ='auto')\n",
    "\n",
    "##########################################################\n",
    "#### removing spurious data\n",
    "mask = df['LDH'] == 'Ip<Ip_MIN'\n",
    "df_filter = df.drop(index = df[mask].index) #remove Ip<Ip_MIN values \n",
    "\n",
    "df_filter = df_filter.dropna() #remove Nan values\n",
    "df_filter = df_filter.reset_index(drop=True) #reset indexing\n",
    "#df_filter.LDH = df_filter.LDH.cat.remove_categories('Ip<Ip_MIN') #remove Ip<Ip_MIN category\n",
    "\n",
    "discard_data = len(df.index) - len(df_filter.index) # number of data point that do not contain useful information\n",
    "print('number of useless data points: ', discard_data)\n",
    "print('size of filtered data set: ', len(df_filter.index))\n",
    "print('size of original data set: ', len(df.index))\n",
    "print(len(df_filter.index) + discard_data - len(df.index))\n",
    "\n",
    "##########################################################\n",
    "# separation into experiments\n",
    "# contruction of labels to numerical values\n",
    "\n",
    "total_samples = 0\n",
    "counter = 0 \n",
    "window_size = 20\n",
    "num_pulses=int(max(df_filter['pulse'].values)) # tot number of different pulses == tot number of different experiments\n",
    "\n",
    "number_correct_samples = 127832\n",
    "all_samples  = np.zeros(127832 * 20 * 4).reshape((-1, 20, 4))\n",
    "all_labels = np.zeros(127832 * 3).reshape((-1, 3))\n",
    "\n",
    "for k in range(num_pulses):    \n",
    "    print('running experiment ', k+1 )\n",
    "    mask_experiment = df_filter.pulse == k + 1\n",
    "    df_experiment = df_filter[mask_experiment]\n",
    "    df_experiment = df_experiment.reset_index(drop=True)    \n",
    "    \n",
    "    # labels\n",
    "    maskl = df_experiment.LDH == 'L'\n",
    "    maskd = df_experiment.LDH == 'D'\n",
    "    maskh = df_experiment.LDH == 'H'\n",
    "    labels = np.vstack((maskl, maskd, maskh)).T + 0.0\n",
    "    \n",
    "    features_exp = df_experiment.keys().to_numpy()\n",
    "    mask_features_exp = np.array([False, True, True, True, True, False, False ])\n",
    "    features_exp = features_exp[mask_features_exp]\n",
    "    x_exp = np.array( df_experiment.loc[:, features_exp].values )    \n",
    "\n",
    "    # this number varies from one experiment to another\n",
    "    # total number of samples we can get from each experiment given\n",
    "    # the time window\n",
    "    num_samples = int( x_exp.shape[0]/window_size )\n",
    "    step = 0\n",
    "    \n",
    "    for i in range(num_samples-):\n",
    "        all_samples[i + counter] = x_exp[ step : step + window_size, : ].reshape((-1, 20, 4)) \n",
    "        all_labels[i + counter] = labels[ step : step + window_size, : ].mean(axis = 0).reshape((-1, 1, 3))                \n",
    "        step += window_size\n",
    "        \n",
    "    counter +=num_samples\n",
    "    total_samples  += num_samples\n",
    "    \n",
    "# create of train and test set\n",
    "validation_split = 0.50\n",
    "shuffle_dataset = True\n",
    "random_seed= 0\n",
    "dataset_size = all_samples.shape[0]\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "x_train = all_samples[train_indices]\n",
    "y_train = all_labels[train_indices]\n",
    "\n",
    "x_test = all_samples[test_indices]\n",
    "y_test = all_labels[test_indices]\n",
    "\n",
    "# transform into a tensor\n",
    "x_train = torch.from_numpy(x_train).float().reshape(-1, 1, 20, 4)[: 63000 ]\n",
    "y_train = torch.from_numpy(y_train).float()[: 63000 ]\n",
    "x_test = torch.from_numpy(x_test).float().reshape(-1, 1, 20, 4)[: 63000 ]\n",
    "y_test = torch.from_numpy(y_test).float()[: 63000 ]  \n",
    "\n",
    "print('train set shape: ', x_train.shape)\n",
    "print('test set shape: ', x_test.shape)\n",
    "\n",
    "# counting the number of each class that we have in our set\n",
    "label_class_1 = y_train == torch.tensor([1., 0., 0.]).view(-1,3)\n",
    "label_class_2 = y_train == torch.tensor([0., 1., 0.]).view(-1,3) \n",
    "label_class_3 = y_train == torch.tensor([0., 0., 1.]).view(-1,3) \n",
    "label_transition = y_train < torch.tensor([1., 1., 1.]).view(-1,3)\n",
    "\n",
    "label_class_1 = label_class_1 + 0.0\n",
    "label_class_2 = label_class_2 + 0.0\n",
    "label_class_3 = label_class_3 + 0.0\n",
    "label_transition = label_transition + 0.0\n",
    "\n",
    "total_class1 = label_class_1.prod(dim = 1).mean()\n",
    "total_class2 = label_class_2.prod(dim = 1).mean()\n",
    "total_class3 = label_class_3.prod(dim = 1).mean()\n",
    "total_transition= label_transition.prod(dim = 1).mean()\n",
    "\n",
    "print('class 1 total percent: ', total_class1*100)\n",
    "print('class 2 total percent: ', total_class2*100)\n",
    "print('class 3 total percent: ', total_class3*100)\n",
    "print('transition total percent: ', total_transition*100)\n",
    "\n",
    "### info regarding data shape for models to work \n",
    "'''\n",
    "    train or test data must be torch.Size([1000, 1, 28, 28])\n",
    "    size explanation tensor ( [ # N_samples , # channels_input, hight, width ] )                               \n",
    "                       \n",
    "    target or label tensor \n",
    "    torch.Size([N_samples, # labels/classes])\n",
    "                                   \n",
    "'''\n",
    "#%%\n",
    "######################################################################\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=2, padding =1, padding_mode = 'replicate' ),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=2, padding =1, padding_mode = 'replicate' ),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=2, padding =1, padding_mode = 'replicate' ),            \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.fc1 = nn.Linear( 32 * 6 *2 , 2 * nb_hidden)\n",
    "        self.fc2 = nn.Linear( 2 * nb_hidden , nb_hidden )\n",
    "        self.fc3 = nn.Linear( nb_hidden , nb_hidden )\n",
    "        self.fc4 = nn.Linear( nb_hidden, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # ouput shape torch.Size([100, 16, 10, 2])\n",
    "        x = self.conv2(x) # ouput shape torch.Size([100, 16, 5, 1])\n",
    "        x = self.conv3(x) # ouput shape torch.Size([100, 32, 3, 1])        \n",
    "        x = x.view(-1, 32 * 6 *2 )\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)         \n",
    "        return x\n",
    "        #return F.softmax(x, dim = 1)\n",
    "    \n",
    "######################################################################\n",
    "\n",
    "def train_model(model, train_input, train_target, mini_batch_size, acc_loss_vector, nb_epochs = 10):\n",
    "    lr = 1e-4\n",
    "    criterion = nn.CrossEntropyLoss()        \n",
    "    #optimizer = optim.SGD(model.parameters(), lr = lr,  momentum=0.9)    \n",
    "    optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.9), lr=lr)\n",
    "    #acc_loss_vector = torch.zeros(nb_epochs) \n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0          \n",
    "        for b in range(0, train_input.size(0), mini_batch_size):            \n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))            \n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size).squeeze())\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc_loss_vector[e] = acc_loss\n",
    "        print('epoch: ' +  str(e) + ', loss: ' + str(acc_loss) )\n",
    "    \n",
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):    \n",
    "    nb_data_errors = 0    \n",
    "    with torch.no_grad():\n",
    "        for b in range(0, data_input.size(0), mini_batch_size):\n",
    "            output = model(data_input.narrow(0, b, mini_batch_size))    \n",
    "            _, predicted = torch.max(output, 1)    \n",
    "            for k in range(mini_batch_size):\n",
    "                if data_target[b + k].view(-1, 3).max(1)[1]  != predicted[k] :\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "    return nb_data_errors            \n",
    "\n",
    "#######################################################################\n",
    "mini_batch_size = 1000\n",
    "nb_hidden = 200\n",
    "\n",
    "# to train in GPU if available \n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cuda')\n",
    "# print('working device ', device)\n",
    "# Net2.to(device)\n",
    "# x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "# # train with H states only\n",
    "# label_class_3_train = y_train == torch.tensor([0., 0., 1.]).view(-1,3) \n",
    "# label_class_3_train = label_class_3_train + 0.0\n",
    "# mask_H = label_class_3_train.prod(dim = 1) > 0 \n",
    "\n",
    "# # data to feed model\n",
    "# data_train_H = x_train[mask_H] [: 20000]\n",
    "# labels_train_H = y_train[mask_H] [: 20000]\n",
    "\n",
    "# model training\n",
    "nb_epochs = 10\n",
    "acc_loss_vector = torch.zeros(nb_epochs) \n",
    "model = Net(nb_hidden)\n",
    "model.train()\n",
    "print('training started')\n",
    "train_model(model, x_train, y_train, mini_batch_size, acc_loss_vector)\n",
    "print('training finished')\n",
    "\n",
    "# data to test model accuracy\n",
    "label_class_2_test = y_test == torch.tensor([0., 1., 0.]).view(-1,3) \n",
    "label_class_2_test = label_class_2_test + 0.0\n",
    "mask_H_test = label_class_2_test.prod(dim = 1) > 0 \n",
    "x_test_H = x_test[mask_H_test][: 3000]\n",
    "y_test_H = y_test[mask_H_test][: 3000]\n",
    "\n",
    "# model(data_test_H)\n",
    "#%%\n",
    "#######################################################################\n",
    "nb_train_errors = compute_nb_errors(model, x_train, y_train, mini_batch_size) / x_train.size(0) * 100\n",
    "print('train error: ', str(nb_train_errors) + '%')\n",
    "\n",
    "nb_test_errors = compute_nb_errors(model, x_test, y_test, mini_batch_size) / x_test.size(0) * 100\n",
    "print('test error: ' +str(nb_test_errors) + '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36004ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a2b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbcf81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee1e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
