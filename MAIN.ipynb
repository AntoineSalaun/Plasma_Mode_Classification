{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa66c58",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf47afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# data packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# torch libraries\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "# import dlc_practical_prologue as prologue\n",
    "# train_input, train_target, test_input, test_target = \\\n",
    "#     prologue.load_data(one_hot_labels = True, normalize = True, flatten = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba36397",
   "metadata": {},
   "source": [
    "# SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162f120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data splitting\n",
    "def split_data(x, y, ratio=0.90, seed=0):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    dataset_size = x.shape[0]\n",
    "    indices = np.random.permutation(dataset_size)\n",
    "    threshold  = int(ratio * dataset_size)\n",
    "    index_train = indices[:threshold]\n",
    "    index_test = indices[threshold:]\n",
    "    # create split\n",
    "    x_training = x[index_train]\n",
    "    x_test = x[index_test]\n",
    "    y_training = y[index_train]\n",
    "    y_test = y[index_test]\n",
    "    return x_training, x_test, y_training, y_test\n",
    "\n",
    "#### data importing \n",
    "parquet_file = 'TCV_LHD_db4ML.parquet.part'\n",
    "df = pd.read_parquet(parquet_file, engine ='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7e9302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .cat accessor with a 'category' dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kc/5wgtb93s7y9d5mzpdd41qclw0000gn/T/ipykernel_1210/2423039089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#remove Nan values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#reset indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLDH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLDH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ip<Ip_MIN'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#remove Ip<Ip_MIN category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2601\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   2608\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2610\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .cat accessor with a 'category' dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_delegate_property_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .cat accessor with a 'category' dtype"
     ]
    }
   ],
   "source": [
    "#### removing spurious data\n",
    "mask = df['LDH'] == 'Ip<Ip_MIN'\n",
    "df_filter = df.drop(index = df[mask].index) #remove Ip<Ip_MIN values \n",
    "\n",
    "df_filter = df_filter.dropna() #remove Nan values\n",
    "df_filter = df_filter.reset_index(drop=True) #reset indexing\n",
    "df_filter.LDH = df_filter.LDH.cat.remove_categories('Ip<Ip_MIN') #remove Ip<Ip_MIN category\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "discard_data = len(df.index) - len(df_filter.index) # number of data point that do not contain useful information\n",
    "print('number of useless data points: ', discard_data)\n",
    "print('size of filtered data set: ', len(df_filter.index))\n",
    "print('size of original data set: ', len(df.index))\n",
    "print(len(df_filter.index) + discard_data - len(df.index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579d524",
   "metadata": {},
   "source": [
    "# Reshaping the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ab207a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##########################################################\n",
    "# separation into experiments\n",
    "# contruction of labels to numerical values\n",
    "\n",
    "total_samples = 0\n",
    "counter = 0 \n",
    "window_size = 20\n",
    "num_pulses=int(max(df_filter['pulse'].values)) # tot number of different pulses == tot number of different experiments\n",
    "\n",
    "number_correct_samples = 127832\n",
    "all_samples  = np.zeros(127832 * 20 * 4).reshape((-1, 20, 4))\n",
    "all_labels = np.zeros(127832 * 3).reshape((-1, 3))\n",
    "\n",
    "for k in range(num_pulses-1):    \n",
    "    #print('running experiment ', k+1 )\n",
    "    mask_experiment = df_filter.pulse == k + 1\n",
    "    df_experiment = df_filter[mask_experiment]\n",
    "    df_experiment = df_experiment.reset_index(drop=True)    \n",
    "    \n",
    "    # labels\n",
    "    maskl = df_experiment.LDH == 'L'\n",
    "    maskd = df_experiment.LDH == 'D'\n",
    "    maskh = df_experiment.LDH == 'H'\n",
    "    labels = np.vstack((maskl, maskd, maskh)).T + 0.0\n",
    "    \n",
    "    features_exp = df_experiment.keys().to_numpy()\n",
    "    mask_features_exp = np.array([False, True, True, True, True, False, False ])\n",
    "    features_exp = features_exp[mask_features_exp]\n",
    "    x_exp = np.array( df_experiment.loc[:, features_exp].values )    \n",
    "\n",
    "    # this number varies from one experiment to another  \n",
    "    num_samples = int( x_exp.shape[0]/window_size )\n",
    "    step = 0\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        all_samples[i + counter] = x_exp[ step : step + window_size, : ].reshape((-1, 20, 4)) \n",
    "        all_labels[i + counter] = labels[ step : step + window_size, : ].mean(axis = 0).reshape((-1, 1, 3))                \n",
    "        step += window_size\n",
    "        \n",
    "    counter +=num_samples\n",
    "    total_samples  += num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a780d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape:  torch.Size([63900, 1, 20, 4])\n",
      "test set shape:  torch.Size([38349, 1, 20, 4])\n"
     ]
    }
   ],
   "source": [
    "# create of train and test set\n",
    "validation_split = 0.30\n",
    "shuffle_dataset = True\n",
    "random_seed= 0\n",
    "dataset_size = all_samples.shape[0]\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "x_train = all_samples[train_indices]\n",
    "y_train = all_labels[train_indices]\n",
    "\n",
    "x_test = all_samples[test_indices]\n",
    "y_test = all_labels[test_indices]\n",
    "\n",
    "# transform into a tensor\n",
    "x_train = torch.from_numpy(x_train).float().reshape(-1, 1, 20, 4)[: 63900 ]\n",
    "y_train = torch.from_numpy(y_train).float()[: 63900 ]\n",
    "x_test = torch.from_numpy(x_test).float().reshape(-1, 1, 20, 4)[: 63900 ]\n",
    "y_test = torch.from_numpy(y_test).float()[: 63900 ]  \n",
    "\n",
    "print('train set shape: ', x_train.shape)\n",
    "print('test set shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c80317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    train or test data must be torch.Size([1000, 1, 28, 28])\\n    size explanation tensor ( [ # N_samples , # channels_input, height, width ] )                               \\n                       \\n    target or label tensor \\n    torch.Size([N_samples, # labels/classes])\\n                                   \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### info regarding data shape for models to work \n",
    "'''\n",
    "    train or test data must be torch.Size([1000, 1, 28, 28])\n",
    "    size explanation tensor ( [ # N_samples , # channels_input, height, width ] )                               \n",
    "                       \n",
    "    target or label tensor \n",
    "    torch.Size([N_samples, # labels/classes])\n",
    "                                   \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f1997",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ac1ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, nb_hidden):        \n",
    "        super().__init__()        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=2, padding =1, padding_mode = 'replicate' )\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=2, padding =1, padding_mode = 'replicate' )\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=2, padding =1, padding_mode = 'replicate' )\n",
    "        \n",
    "        #self.lstm1 = nn.LSTM(64, 64, bidirectional=True, batch_first=True)\n",
    "        #self.lstm2 = nn.LSTM(64 * 2, 64, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear( 64 * 6 * 2 , nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(32) # batch normalization\n",
    "        self.bn3 = nn.BatchNorm2d(64) # batch normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.conv1(x), kernel_size=2)\n",
    "        x = self.bn1(x) \n",
    "        x = torch.relu(x)        \n",
    "        \n",
    "        x = F.max_pool2d(self.conv2(x), kernel_size=2)\n",
    "        x = self.bn1(x)     \n",
    "        x = torch.relu(x)        \n",
    "        \n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)     \n",
    "        x = torch.relu(self.fc1(x.view(-1, 768)))\n",
    "        \n",
    "        x = self.fc2(x) \n",
    "        x = torch.softmax(x, dim = 0)\n",
    "        \n",
    "        #self.lstm1.flatten_parameters()\n",
    "        #self.lstm2.flatten_parameters()\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4c32361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size, nb_epochs = 10):\n",
    "    lr = 1e-8\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = lr)    \n",
    "\n",
    "    for e in tqdm(range(nb_epochs)):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):            \n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a574545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    eps = 0.2 # tolerance\n",
    "    nb_data_errors = 0    \n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))    \n",
    "        #predicted_classes , _ = output.max(dim  = 0)     \n",
    "        for k in range(mini_batch_size):\n",
    "            if torch.norm(data_target[b + k] - output[k])  > eps :\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "    return nb_data_errors            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8da3e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_nb_errors(model, data_input, target, mini_batch_size):\n",
    "#     nb_errors = 0\n",
    "#     for b in range(0, data_input.size(0), mini_batch_size):\n",
    "#         output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "#         _, predicted_classes = output.max(1)\n",
    "#         for k in range(mini_batch_size):\n",
    "#             if target[b + k, predicted_classes[k]] <= 0:\n",
    "#                 nb_errors = nb_errors + 1\n",
    "#     return nb_errors\n",
    "\n",
    "#######################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b1b53e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56d9bb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:32<00:00,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n",
      "Net2(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (lstm1): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (lstm2): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
      "  (fc1): Linear(in_features=768, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "nb_hidden = 10\n",
    "# fake data to test if model runs\n",
    "# torch.manual_seed(100)\n",
    "\n",
    "# to train in GPU if available \n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cuda')\n",
    "# print('working device ', device)\n",
    "# Net2.to(device)\n",
    "# x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "model = Net2(nb_hidden)\n",
    "model.train()\n",
    "print('training started')\n",
    "train_model(model, x_train, y_train, mini_batch_size)\n",
    "print('training finished')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "403b7bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6076e-05, 2.6076e-05, 2.6076e-05], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_test)[6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c1b6b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net2 0.00%% 0/38349\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    eps = 0.2 # tolerance\n",
    "    nb_data_errors = 0    \n",
    "    for b in range(0, data_input.size(0)- mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))    \n",
    "        \n",
    "        \n",
    "        # HERE WE SHOULD USE THE KAPPA FUNCTION\n",
    "        \n",
    "        #predicted_classes , _ = output.max(dim  = 0)     \n",
    "        #print(output)\n",
    "    return nb_data_errors            \n",
    "\n",
    "nb_test_errors = compute_nb_errors(model, x_test, y_test, mini_batch_size)\n",
    "print('test error Net2 {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / x_train.size(0),\n",
    "                                                    nb_test_errors, x_test.size(0)))\n",
    "# test functions\n",
    "# x_test.narrow(0, 0, mini_batch_size).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
