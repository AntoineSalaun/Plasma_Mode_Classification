{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa66c58",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf47afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# data packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# torch libraries\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "# import dlc_practical_prologue as prologue\n",
    "# train_input, train_target, test_input, test_target = \\\n",
    "#     prologue.load_data(one_hot_labels = True, normalize = True, flatten = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba36397",
   "metadata": {},
   "source": [
    "# SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162f120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data splitting\n",
    "def split_data(x, y, ratio=0.90, seed=0):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    dataset_size = x.shape[0]\n",
    "    indices = np.random.permutation(dataset_size)\n",
    "    threshold  = int(ratio * dataset_size)\n",
    "    index_train = indices[:threshold]\n",
    "    index_test = indices[threshold:]\n",
    "    # create split\n",
    "    x_training = x[index_train]\n",
    "    x_test = x[index_test]\n",
    "    y_training = y[index_train]\n",
    "    y_test = y[index_test]\n",
    "    return x_training, x_test, y_training, y_test\n",
    "\n",
    "#### data importing \n",
    "parquet_file = 'TCV_LHD_db4ML.parquet.part'\n",
    "df = pd.read_parquet(parquet_file, engine ='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7e9302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .cat accessor with a 'category' dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kc/5wgtb93s7y9d5mzpdd41qclw0000gn/T/ipykernel_12926/2423039089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#remove Nan values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#reset indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLDH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLDH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ip<Ip_MIN'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#remove Ip<Ip_MIN category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2601\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   2608\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2610\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .cat accessor with a 'category' dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_delegate_property_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .cat accessor with a 'category' dtype"
     ]
    }
   ],
   "source": [
    "#### removing spurious data\n",
    "mask = df['LDH'] == 'Ip<Ip_MIN'\n",
    "df_filter = df.drop(index = df[mask].index) #remove Ip<Ip_MIN values \n",
    "\n",
    "df_filter = df_filter.dropna() #remove Nan values\n",
    "df_filter = df_filter.reset_index(drop=True) #reset indexing\n",
    "df_filter.LDH = df_filter.LDH.cat.remove_categories('Ip<Ip_MIN') #remove Ip<Ip_MIN category\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "discard_data = len(df.index) - len(df_filter.index) # number of data point that do not contain useful information\n",
    "print('number of useless data points: ', discard_data)\n",
    "print('size of filtered data set: ', len(df_filter.index))\n",
    "print('size of original data set: ', len(df.index))\n",
    "print(len(df_filter.index) + discard_data - len(df.index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4979f2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>IP</th>\n",
       "      <th>PD</th>\n",
       "      <th>FIR</th>\n",
       "      <th>WP</th>\n",
       "      <th>LDH</th>\n",
       "      <th>pulse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0257</td>\n",
       "      <td>50076.167327</td>\n",
       "      <td>1.796861</td>\n",
       "      <td>5.006723e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0258</td>\n",
       "      <td>50372.816198</td>\n",
       "      <td>1.816465</td>\n",
       "      <td>5.024210e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0259</td>\n",
       "      <td>50640.227687</td>\n",
       "      <td>2.031274</td>\n",
       "      <td>5.114125e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>50907.639176</td>\n",
       "      <td>1.894527</td>\n",
       "      <td>5.160487e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0261</td>\n",
       "      <td>51078.462437</td>\n",
       "      <td>1.865220</td>\n",
       "      <td>5.162926e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20070</th>\n",
       "      <td>2.0329</td>\n",
       "      <td>50851.319914</td>\n",
       "      <td>3.149575</td>\n",
       "      <td>7.777638e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071</th>\n",
       "      <td>2.0330</td>\n",
       "      <td>50691.889338</td>\n",
       "      <td>3.354183</td>\n",
       "      <td>7.714125e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20072</th>\n",
       "      <td>2.0331</td>\n",
       "      <td>50440.248135</td>\n",
       "      <td>3.149570</td>\n",
       "      <td>7.642720e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20073</th>\n",
       "      <td>2.0332</td>\n",
       "      <td>50188.412971</td>\n",
       "      <td>3.120116</td>\n",
       "      <td>7.652230e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20074</th>\n",
       "      <td>2.0333</td>\n",
       "      <td>50110.729926</td>\n",
       "      <td>3.300745</td>\n",
       "      <td>7.639677e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20075 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time            IP        PD           FIR   WP LDH  pulse\n",
       "0      0.0257  50076.167327  1.796861  5.006723e+18  0.0   L    1.0\n",
       "1      0.0258  50372.816198  1.816465  5.024210e+18  0.0   L    1.0\n",
       "2      0.0259  50640.227687  2.031274  5.114125e+18  0.0   L    1.0\n",
       "3      0.0260  50907.639176  1.894527  5.160487e+18  0.0   L    1.0\n",
       "4      0.0261  51078.462437  1.865220  5.162926e+18  0.0   L    1.0\n",
       "...       ...           ...       ...           ...  ...  ..    ...\n",
       "20070  2.0329  50851.319914  3.149575  7.777638e+18  0.0   L    1.0\n",
       "20071  2.0330  50691.889338  3.354183  7.714125e+18  0.0   L    1.0\n",
       "20072  2.0331  50440.248135  3.149570  7.642720e+18  0.0   L    1.0\n",
       "20073  2.0332  50188.412971  3.120116  7.652230e+18  0.0   L    1.0\n",
       "20074  2.0333  50110.729926  3.300745  7.639677e+18  0.0   L    1.0\n",
       "\n",
       "[20075 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter[df_filter['pulse'] == 1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82e2d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "def create_freq_PD(data_frame):\n",
    "    fs = 1e4\n",
    "    \n",
    "    frequency_extended = np.array([]) \n",
    "    \n",
    "    for i in range(1,1+int(max(df_filter['pulse'].values))):\n",
    "        #print(i)\n",
    "        f, t, Sxx = signal.spectrogram(df_filter[df_filter['pulse'] == i]['PD'], fs,nperseg = 512, noverlap = 64 )\n",
    "        #plt.pcolormesh(t, f, Sxx, shading='gouraud')\n",
    "        #plt.ylabel('Frequency [Hz]')\n",
    "        #plt.xlabel('Time [sec]')\n",
    "        #plt.show\n",
    "        avg = np.zeros((Sxx.shape[1]))\n",
    "        avg = np.mean(Sxx, axis=0)\n",
    "        t.shape\n",
    "        current = np.resize(avg,df_filter[df_filter['pulse'] == i]['time'].shape)\n",
    "    \n",
    "        \n",
    "        frequency_extended = np.concatenate((frequency_extended,current))\n",
    "        \n",
    "    \n",
    "    data_frame['PD_freq'] = frequency_extended\n",
    "\n",
    "create_freq_PD(df_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eab45c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>IP</th>\n",
       "      <th>PD</th>\n",
       "      <th>FIR</th>\n",
       "      <th>WP</th>\n",
       "      <th>LDH</th>\n",
       "      <th>pulse</th>\n",
       "      <th>PD_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0257</td>\n",
       "      <td>50076.167327</td>\n",
       "      <td>1.796861</td>\n",
       "      <td>5.006723e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0258</td>\n",
       "      <td>50372.816198</td>\n",
       "      <td>1.816465</td>\n",
       "      <td>5.024210e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0259</td>\n",
       "      <td>50640.227687</td>\n",
       "      <td>2.031274</td>\n",
       "      <td>5.114125e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>50907.639176</td>\n",
       "      <td>1.894527</td>\n",
       "      <td>5.160487e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0261</td>\n",
       "      <td>51078.462437</td>\n",
       "      <td>1.865220</td>\n",
       "      <td>5.162926e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559093</th>\n",
       "      <td>1.6629</td>\n",
       "      <td>-132822.096678</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-1.714051e+19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559094</th>\n",
       "      <td>1.6630</td>\n",
       "      <td>-108659.484245</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-1.790255e+19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559095</th>\n",
       "      <td>1.6631</td>\n",
       "      <td>-86085.879530</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-1.880538e+19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559096</th>\n",
       "      <td>1.6633</td>\n",
       "      <td>-45424.997785</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-1.626079e+19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559097</th>\n",
       "      <td>1.6634</td>\n",
       "      <td>-30335.058768</td>\n",
       "      <td>5.890288</td>\n",
       "      <td>-2.196298e+19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2559098 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time             IP         PD           FIR   WP LDH  pulse  \\\n",
       "0        0.0257   50076.167327   1.796861  5.006723e+18  0.0   L    1.0   \n",
       "1        0.0258   50372.816198   1.816465  5.024210e+18  0.0   L    1.0   \n",
       "2        0.0259   50640.227687   2.031274  5.114125e+18  0.0   L    1.0   \n",
       "3        0.0260   50907.639176   1.894527  5.160487e+18  0.0   L    1.0   \n",
       "4        0.0261   51078.462437   1.865220  5.162926e+18  0.0   L    1.0   \n",
       "...         ...            ...        ...           ...  ...  ..    ...   \n",
       "2559093  1.6629 -132822.096678  10.000000 -1.714051e+19  0.0   L  172.0   \n",
       "2559094  1.6630 -108659.484245  10.000000 -1.790255e+19  0.0   L  172.0   \n",
       "2559095  1.6631  -86085.879530  10.000000 -1.880538e+19  0.0   L  172.0   \n",
       "2559096  1.6633  -45424.997785  10.000000 -1.626079e+19  0.0   L  172.0   \n",
       "2559097  1.6634  -30335.058768   5.890288 -2.196298e+19  0.0   L  172.0   \n",
       "\n",
       "          PD_freq  \n",
       "0        0.000001  \n",
       "1        0.000009  \n",
       "2        0.000002  \n",
       "3        0.000007  \n",
       "4        0.000090  \n",
       "...           ...  \n",
       "2559093  0.000015  \n",
       "2559094  0.000029  \n",
       "2559095  0.000025  \n",
       "2559096  0.000027  \n",
       "2559097  0.000027  \n",
       "\n",
       "[2559098 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579d524",
   "metadata": {},
   "source": [
    "# Reshaping the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12ab207a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##########################################################\n",
    "# separation into experiments\n",
    "# contruction of labels to numerical values\n",
    "\n",
    "total_samples = 0\n",
    "counter = 0 \n",
    "window_size = 40\n",
    "num_features = 5\n",
    "num_pulses=int(max(df_filter['pulse'].values)) # tot number of different pulses == tot number of different experiments\n",
    "\n",
    "number_correct_samples = 127832\n",
    "all_samples  = np.zeros(127832 * window_size * num_features).reshape((-1, window_size, num_features))\n",
    "all_labels = np.zeros(127832 * 3).reshape((-1, 3))\n",
    "\n",
    "for k in range(num_pulses):    \n",
    "    #print('running experiment ', k+1 )\n",
    "    mask_experiment = df_filter.pulse == k + 1\n",
    "    df_experiment = df_filter[mask_experiment]\n",
    "    df_experiment = df_experiment.reset_index(drop=True)    \n",
    "    \n",
    "    # labels\n",
    "    maskl = df_experiment.LDH == 'L'\n",
    "    maskd = df_experiment.LDH == 'D'\n",
    "    maskh = df_experiment.LDH == 'H'\n",
    "    labels = np.vstack((maskl, maskd, maskh)).T + 0.0\n",
    "    \n",
    "    features_exp = df_experiment.keys().to_numpy()\n",
    "    mask_features_exp = np.array([False, True, True, True, True, False, False, True ])\n",
    "    features_exp = features_exp[mask_features_exp]\n",
    "    x_exp = np.array( df_experiment.loc[:, features_exp].values )    \n",
    "\n",
    "    # this number varies from one experiment to another  \n",
    "    num_samples = int( x_exp.shape[0]/window_size )\n",
    "    step = 0\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        all_samples[i + counter] = x_exp[ step : step + window_size, : ].reshape((-1, window_size, num_features)) \n",
    "        all_labels[i + counter] = labels[ step : step + window_size, : ].mean(axis = 0).reshape((-1, 1, 3))                \n",
    "        step += window_size\n",
    "        \n",
    "    counter +=num_samples\n",
    "    total_samples  += num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7b02d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9381, 40, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def balance_subset(samples,labels):\n",
    "\n",
    "    number_of_L = np.sum(labels, axis=0)\n",
    "    scale = np.floor(min(number_of_L))\n",
    "\n",
    "    L_rows = np.where(labels[:,0]==1)\n",
    "    D_rows = np.where(labels[:,1]==1)\n",
    "    H_rows = np.where(labels[:,2]==1)\n",
    "    \n",
    "    minimum = min(len(L_rows[0]),len(D_rows[0]),len(H_rows[0]))\n",
    "    \n",
    "    sub_L = np.random.choice(L_rows[0] , minimum)\n",
    "    sub_D = np.random.choice(D_rows[0] , minimum)\n",
    "    sub_H = np.random.choice(H_rows[0] , minimum)\n",
    "    \n",
    "    rows=np.concatenate((sub_L, sub_D,sub_H), axis=None)\n",
    "    len(rows)/3\n",
    "    \n",
    "    balanced_labels = labels[rows]\n",
    "    balanced_samples = samples[rows]\n",
    "    \n",
    "    return balanced_samples, balanced_labels\n",
    "\n",
    "balanced_x, balanced_y = balance_subset(all_samples,all_labels)\n",
    "balanced_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a780d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create of train and test set\n",
    "validation_split = 0.30\n",
    "shuffle_dataset = True\n",
    "random_seed= 0\n",
    "dataset_size = all_samples.shape[0]\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "x_train = all_samples[train_indices]\n",
    "y_train = all_labels[train_indices]\n",
    "\n",
    "x_test = all_samples[test_indices]\n",
    "y_test = all_labels[test_indices]\n",
    "\n",
    "df_test = pd.DataFrame(x_test[:, 0, 0])\n",
    "\n",
    "# transform into a tensor\n",
    "x_train = torch.from_numpy(x_train).float().reshape(-1, 1, window_size, num_features)[: 9000 ]\n",
    "y_train = torch.from_numpy(y_train).float()[: 9000 ]\n",
    "x_test = torch.from_numpy(x_test).float().reshape(-1, 1, window_size, num_features)[: 9000 ]\n",
    "y_test = torch.from_numpy(y_test).float()[: 9000 ]  \n",
    "\n",
    "#print('train set shape: ', x_train.shape)\n",
    "#print('test set shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23c80317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    train or test data must be torch.Size([1000, 1, 28, 28])\\n    size explanation tensor ( [ # N_samples , # channels_input, height, width ] )                               \\n                       \\n    target or label tensor \\n    torch.Size([N_samples, # labels/classes])\\n                                   \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### info regarding data shape for models to work \n",
    "'''\n",
    "    train or test data must be torch.Size([1000, 1, 28, 28])\n",
    "    size explanation tensor ( [ # N_samples , # channels_input, height, width ] )                               \n",
    "                       \n",
    "    target or label tensor \n",
    "    torch.Size([N_samples, # labels/classes])\n",
    "                                   \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f1997",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ac1ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, nb_hidden):        \n",
    "        super().__init__()        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=2, padding =1, padding_mode = 'replicate' )\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=2, padding =1, padding_mode = 'replicate' )\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=2, padding =1, padding_mode = 'replicate' )\n",
    "        \n",
    "        #self.lstm1 = nn.LSTM(64, 64, bidirectional=True, batch_first=True)\n",
    "        #self.lstm2 = nn.LSTM(64 * 2, 64, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear( 64 * 6 * 2 , nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(32) # batch normalization\n",
    "        self.bn3 = nn.BatchNorm2d(64) # batch normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.conv1(x), kernel_size=2)\n",
    "        x = self.bn1(x) \n",
    "        x = torch.relu(x)        \n",
    "        \n",
    "        x = F.max_pool2d(self.conv2(x), kernel_size=2)\n",
    "        x = self.bn1(x)     \n",
    "        x = torch.relu(x)        \n",
    "        \n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)     \n",
    "        x = torch.relu(self.fc1(x.view(-1, 768)))\n",
    "        \n",
    "        x = self.fc2(x) \n",
    "        x = torch.softmax(x, dim = 0)\n",
    "        \n",
    "        #self.lstm1.flatten_parameters()\n",
    "        #self.lstm2.flatten_parameters()\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4c32361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size, nb_epochs = 10):\n",
    "    lr = 1e-8\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = lr)    \n",
    "\n",
    "    for e in tqdm(range(nb_epochs)):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):            \n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a574545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    eps = 0.2 # tolerance\n",
    "    nb_data_errors = 0    \n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))    \n",
    "        #predicted_classes , _ = output.max(dim  = 0)     \n",
    "        for k in range(mini_batch_size):\n",
    "            if torch.norm(data_target[b + k] - output[k])  > eps :\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "    return nb_data_errors            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8da3e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_nb_errors(model, data_input, target, mini_batch_size):\n",
    "#     nb_errors = 0\n",
    "#     for b in range(0, data_input.size(0), mini_batch_size):\n",
    "#         output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "#         _, predicted_classes = output.max(1)\n",
    "#         for k in range(mini_batch_size):\n",
    "#             if target[b + k, predicted_classes[k]] <= 0:\n",
    "#                 nb_errors = nb_errors + 1\n",
    "#     return nb_errors\n",
    "\n",
    "#######################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56d9bb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (275) to match target batch_size (100).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kc/5wgtb93s7y9d5mzpdd41qclw0000gn/T/ipykernel_12926/2220116899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training started'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kc/5wgtb93s7y9d5mzpdd41qclw0000gn/T/ipykernel_12926/2276238939.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_input, train_target, mini_batch_size, nb_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (275) to match target batch_size (100)."
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "nb_hidden = 10\n",
    "# fake data to test if model runs\n",
    "# torch.manual_seed(100)\n",
    "\n",
    "# to train in GPU if available \n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cuda')\n",
    "# print('working device ', device)\n",
    "# Net2.to(device)\n",
    "# x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "model = Net2(nb_hidden)\n",
    "model.train()\n",
    "print('training started')\n",
    "train_model(model, x_train, y_train, mini_batch_size)\n",
    "print('training finished')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "403b7bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6076e-05, 2.6076e-05, 2.6076e-05], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e80835c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38349, 1, 20, 4])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 660\n",
    "model(x_test).shape\n",
    "\n",
    "x_test_2 = x_test.reshape(-1, 1, 20, 4)[: 63900 ]\n",
    "\n",
    "x_test.shape\n",
    "#df_X_test = pd.DataFrame(x_test.numpy())\n",
    "#x_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c1b6b86",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2751501328.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/kc/5wgtb93s7y9d5mzpdd41qclw0000gn/T/ipykernel_8281/2751501328.py\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    for b in range(0, data_input.size(0, mini_batch_size):\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    eps = 0.2 # tolerance\n",
    "    nb_data_errors = 0    \n",
    "    for b in range(0, data_input.size(0)- mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))    \n",
    "        \n",
    "        \n",
    "        # HERE WE SHOULD USE THE KAPPA FUNCTION\n",
    "        \n",
    "        #predicted_classes , _ = output.max(dim  = 0)     \n",
    "        #print(output)\n",
    "    return nb_data_errors \n",
    "\n",
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):    \n",
    "    nb_data_errors = 0    \n",
    "    with torch.no_grad():\n",
    "        for b in range(0, data_input.size(0), mini_batch_size):\n",
    "            output = model(data_input.narrow(0, b, mini_batch_size))    \n",
    "            _, predicted = torch.max(output, 1)    \n",
    "            for k in range(mini_batch_size):\n",
    "                if data_target[b + k].view(-1, 3).max(1)[1]  != predicted[k] :\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "    return nb_data_errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def probability_to_category(prediction):\n",
    "    prediction['prediction'] = prediction[['H','L','D']].idxmax(axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "nb_test_errors = compute_nb_errors(model, x_test, y_test, mini_batch_size)\n",
    "print('test error Net2 {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / x_train.size(0),\n",
    "                                                    nb_test_errors, x_test.size(0)))\n",
    "# test functions\n",
    "# x_test.narrow(0, 0, mini_batch_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b2351f6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of useless data points:  56373\n",
      "size of filtered data set:  2559098\n",
      "size of original data set:  2615471\n",
      "0\n",
      "running experiment  1\n",
      "running experiment  2\n",
      "running experiment  3\n",
      "running experiment  4\n",
      "running experiment  5\n",
      "running experiment  6\n",
      "running experiment  7\n",
      "running experiment  8\n",
      "running experiment  9\n",
      "running experiment  10\n",
      "running experiment  11\n",
      "running experiment  12\n",
      "running experiment  13\n",
      "running experiment  14\n",
      "running experiment  15\n",
      "running experiment  16\n",
      "running experiment  17\n",
      "running experiment  18\n",
      "running experiment  19\n",
      "running experiment  20\n",
      "running experiment  21\n",
      "running experiment  22\n",
      "running experiment  23\n",
      "running experiment  24\n",
      "running experiment  25\n",
      "running experiment  26\n",
      "running experiment  27\n",
      "running experiment  28\n",
      "running experiment  29\n",
      "running experiment  30\n",
      "running experiment  31\n",
      "running experiment  32\n",
      "running experiment  33\n",
      "running experiment  34\n",
      "running experiment  35\n",
      "running experiment  36\n",
      "running experiment  37\n",
      "running experiment  38\n",
      "running experiment  39\n",
      "running experiment  40\n",
      "running experiment  41\n",
      "running experiment  42\n",
      "running experiment  43\n",
      "running experiment  44\n",
      "running experiment  45\n",
      "running experiment  46\n",
      "running experiment  47\n",
      "running experiment  48\n",
      "running experiment  49\n",
      "running experiment  50\n",
      "running experiment  51\n",
      "running experiment  52\n",
      "running experiment  53\n",
      "running experiment  54\n",
      "running experiment  55\n",
      "running experiment  56\n",
      "running experiment  57\n",
      "running experiment  58\n",
      "running experiment  59\n",
      "running experiment  60\n",
      "running experiment  61\n",
      "running experiment  62\n",
      "running experiment  63\n",
      "running experiment  64\n",
      "running experiment  65\n",
      "running experiment  66\n",
      "running experiment  67\n",
      "running experiment  68\n",
      "running experiment  69\n",
      "running experiment  70\n",
      "running experiment  71\n",
      "running experiment  72\n",
      "running experiment  73\n",
      "running experiment  74\n",
      "running experiment  75\n",
      "running experiment  76\n",
      "running experiment  77\n",
      "running experiment  78\n",
      "running experiment  79\n",
      "running experiment  80\n",
      "running experiment  81\n",
      "running experiment  82\n",
      "running experiment  83\n",
      "running experiment  84\n",
      "running experiment  85\n",
      "running experiment  86\n",
      "running experiment  87\n",
      "running experiment  88\n",
      "running experiment  89\n",
      "running experiment  90\n",
      "running experiment  91\n",
      "running experiment  92\n",
      "running experiment  93\n",
      "running experiment  94\n",
      "running experiment  95\n",
      "running experiment  96\n",
      "running experiment  97\n",
      "running experiment  98\n",
      "running experiment  99\n",
      "running experiment  100\n",
      "running experiment  101\n",
      "running experiment  102\n",
      "running experiment  103\n",
      "running experiment  104\n",
      "running experiment  105\n",
      "running experiment  106\n",
      "running experiment  107\n",
      "running experiment  108\n",
      "running experiment  109\n",
      "running experiment  110\n",
      "running experiment  111\n",
      "running experiment  112\n",
      "running experiment  113\n",
      "running experiment  114\n",
      "running experiment  115\n",
      "running experiment  116\n",
      "running experiment  117\n",
      "running experiment  118\n",
      "running experiment  119\n",
      "running experiment  120\n",
      "running experiment  121\n",
      "running experiment  122\n",
      "running experiment  123\n",
      "running experiment  124\n",
      "running experiment  125\n",
      "running experiment  126\n",
      "running experiment  127\n",
      "running experiment  128\n",
      "running experiment  129\n",
      "running experiment  130\n",
      "running experiment  131\n",
      "running experiment  132\n",
      "running experiment  133\n",
      "running experiment  134\n",
      "running experiment  135\n",
      "running experiment  136\n",
      "running experiment  137\n",
      "running experiment  138\n",
      "running experiment  139\n",
      "running experiment  140\n",
      "running experiment  141\n",
      "running experiment  142\n",
      "running experiment  143\n",
      "running experiment  144\n",
      "running experiment  145\n",
      "running experiment  146\n",
      "running experiment  147\n",
      "running experiment  148\n",
      "running experiment  149\n",
      "running experiment  150\n",
      "running experiment  151\n",
      "running experiment  152\n",
      "running experiment  153\n",
      "running experiment  154\n",
      "running experiment  155\n",
      "running experiment  156\n",
      "running experiment  157\n",
      "running experiment  158\n",
      "running experiment  159\n",
      "running experiment  160\n",
      "running experiment  161\n",
      "running experiment  162\n",
      "running experiment  163\n",
      "running experiment  164\n",
      "running experiment  165\n",
      "running experiment  166\n",
      "running experiment  167\n",
      "running experiment  168\n",
      "running experiment  169\n",
      "running experiment  170\n",
      "running experiment  171\n",
      "running experiment  172\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 127832 is out of bounds for axis 0 with size 127832",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kc/5wgtb93s7y9d5mzpdd41qclw0000gn/T/ipykernel_8281/1369819677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mall_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_exp\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mall_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 127832 is out of bounds for axis 0 with size 127832"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# data packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# torch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "# import dlc_practical_prologue as prologue\n",
    "# train_input, train_target, test_input, test_target = \\\n",
    "#     prologue.load_data(one_hot_labels = True, normalize = True, flatten = False)\n",
    "\n",
    "##########################################################\n",
    "### data splitting\n",
    "def split_data(x, y, ratio=0.90, seed=0):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    dataset_size = x.shape[0]\n",
    "    indices = np.random.permutation(dataset_size)\n",
    "    threshold  = int(ratio * dataset_size)\n",
    "    index_train = indices[:threshold]\n",
    "    index_test = indices[threshold:]\n",
    "    # create split\n",
    "    x_training = x[index_train]\n",
    "    x_test = x[index_test]\n",
    "    y_training = y[index_train]\n",
    "    y_test = y[index_test]\n",
    "    return x_training, x_test, y_training, y_test\n",
    "\n",
    "#### data importing \n",
    "parquet_file = 'TCV_LHD_db4ML.parquet.part'\n",
    "df = pd.read_parquet(parquet_file, engine ='auto')\n",
    "\n",
    "##########################################################\n",
    "#### removing spurious data\n",
    "mask = df['LDH'] == 'Ip<Ip_MIN'\n",
    "df_filter = df.drop(index = df[mask].index) #remove Ip<Ip_MIN values \n",
    "\n",
    "df_filter = df_filter.dropna() #remove Nan values\n",
    "df_filter = df_filter.reset_index(drop=True) #reset indexing\n",
    "#df_filter.LDH = df_filter.LDH.cat.remove_categories('Ip<Ip_MIN') #remove Ip<Ip_MIN category\n",
    "\n",
    "discard_data = len(df.index) - len(df_filter.index) # number of data point that do not contain useful information\n",
    "print('number of useless data points: ', discard_data)\n",
    "print('size of filtered data set: ', len(df_filter.index))\n",
    "print('size of original data set: ', len(df.index))\n",
    "print(len(df_filter.index) + discard_data - len(df.index))\n",
    "\n",
    "##########################################################\n",
    "# separation into experiments\n",
    "# contruction of labels to numerical values\n",
    "\n",
    "total_samples = 0\n",
    "counter = 0 \n",
    "window_size = 20\n",
    "num_pulses=int(max(df_filter['pulse'].values)) # tot number of different pulses == tot number of different experiments\n",
    "\n",
    "number_correct_samples = 127832\n",
    "all_samples  = np.zeros(127832 * 20 * 4).reshape((-1, 20, 4))\n",
    "all_labels = np.zeros(127832 * 3).reshape((-1, 3))\n",
    "\n",
    "for k in range(num_pulses):    \n",
    "    print('running experiment ', k+1 )\n",
    "    mask_experiment = df_filter.pulse == k + 1\n",
    "    df_experiment = df_filter[mask_experiment]\n",
    "    df_experiment = df_experiment.reset_index(drop=True)    \n",
    "    \n",
    "    # labels\n",
    "    maskl = df_experiment.LDH == 'L'\n",
    "    maskd = df_experiment.LDH == 'D'\n",
    "    maskh = df_experiment.LDH == 'H'\n",
    "    labels = np.vstack((maskl, maskd, maskh)).T + 0.0\n",
    "    \n",
    "    features_exp = df_experiment.keys().to_numpy()\n",
    "    mask_features_exp = np.array([False, True, True, True, True, False, False ])\n",
    "    features_exp = features_exp[mask_features_exp]\n",
    "    x_exp = np.array( df_experiment.loc[:, features_exp].values )    \n",
    "\n",
    "    # this number varies from one experiment to another\n",
    "    # total number of samples we can get from each experiment given\n",
    "    # the time window\n",
    "    num_samples = int( x_exp.shape[0]/window_size )\n",
    "    step = 0\n",
    "    \n",
    "    for i in range(num_samples-):\n",
    "        all_samples[i + counter] = x_exp[ step : step + window_size, : ].reshape((-1, 20, 4)) \n",
    "        all_labels[i + counter] = labels[ step : step + window_size, : ].mean(axis = 0).reshape((-1, 1, 3))                \n",
    "        step += window_size\n",
    "        \n",
    "    counter +=num_samples\n",
    "    total_samples  += num_samples\n",
    "    \n",
    "# create of train and test set\n",
    "validation_split = 0.50\n",
    "shuffle_dataset = True\n",
    "random_seed= 0\n",
    "dataset_size = all_samples.shape[0]\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "x_train = all_samples[train_indices]\n",
    "y_train = all_labels[train_indices]\n",
    "\n",
    "x_test = all_samples[test_indices]\n",
    "y_test = all_labels[test_indices]\n",
    "\n",
    "# transform into a tensor\n",
    "x_train = torch.from_numpy(x_train).float().reshape(-1, 1, 20, 4)[: 63000 ]\n",
    "y_train = torch.from_numpy(y_train).float()[: 63000 ]\n",
    "x_test = torch.from_numpy(x_test).float().reshape(-1, 1, 20, 4)[: 63000 ]\n",
    "y_test = torch.from_numpy(y_test).float()[: 63000 ]  \n",
    "\n",
    "print('train set shape: ', x_train.shape)\n",
    "print('test set shape: ', x_test.shape)\n",
    "\n",
    "# counting the number of each class that we have in our set\n",
    "label_class_1 = y_train == torch.tensor([1., 0., 0.]).view(-1,3)\n",
    "label_class_2 = y_train == torch.tensor([0., 1., 0.]).view(-1,3) \n",
    "label_class_3 = y_train == torch.tensor([0., 0., 1.]).view(-1,3) \n",
    "label_transition = y_train < torch.tensor([1., 1., 1.]).view(-1,3)\n",
    "\n",
    "label_class_1 = label_class_1 + 0.0\n",
    "label_class_2 = label_class_2 + 0.0\n",
    "label_class_3 = label_class_3 + 0.0\n",
    "label_transition = label_transition + 0.0\n",
    "\n",
    "total_class1 = label_class_1.prod(dim = 1).mean()\n",
    "total_class2 = label_class_2.prod(dim = 1).mean()\n",
    "total_class3 = label_class_3.prod(dim = 1).mean()\n",
    "total_transition= label_transition.prod(dim = 1).mean()\n",
    "\n",
    "print('class 1 total percent: ', total_class1*100)\n",
    "print('class 2 total percent: ', total_class2*100)\n",
    "print('class 3 total percent: ', total_class3*100)\n",
    "print('transition total percent: ', total_transition*100)\n",
    "\n",
    "### info regarding data shape for models to work \n",
    "'''\n",
    "    train or test data must be torch.Size([1000, 1, 28, 28])\n",
    "    size explanation tensor ( [ # N_samples , # channels_input, hight, width ] )                               \n",
    "                       \n",
    "    target or label tensor \n",
    "    torch.Size([N_samples, # labels/classes])\n",
    "                                   \n",
    "'''\n",
    "#%%\n",
    "######################################################################\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=2, padding =1, padding_mode = 'replicate' ),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=2, padding =1, padding_mode = 'replicate' ),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=2, padding =1, padding_mode = 'replicate' ),            \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.fc1 = nn.Linear( 32 * 6 *2 , 2 * nb_hidden)\n",
    "        self.fc2 = nn.Linear( 2 * nb_hidden , nb_hidden )\n",
    "        self.fc3 = nn.Linear( nb_hidden , nb_hidden )\n",
    "        self.fc4 = nn.Linear( nb_hidden, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # ouput shape torch.Size([100, 16, 10, 2])\n",
    "        x = self.conv2(x) # ouput shape torch.Size([100, 16, 5, 1])\n",
    "        x = self.conv3(x) # ouput shape torch.Size([100, 32, 3, 1])        \n",
    "        x = x.view(-1, 32 * 6 *2 )\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)         \n",
    "        return x\n",
    "        #return F.softmax(x, dim = 1)\n",
    "    \n",
    "######################################################################\n",
    "\n",
    "def train_model(model, train_input, train_target, mini_batch_size, acc_loss_vector, nb_epochs = 10):\n",
    "    lr = 1e-4\n",
    "    criterion = nn.CrossEntropyLoss()        \n",
    "    #optimizer = optim.SGD(model.parameters(), lr = lr,  momentum=0.9)    \n",
    "    optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.9), lr=lr)\n",
    "    #acc_loss_vector = torch.zeros(nb_epochs) \n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0          \n",
    "        for b in range(0, train_input.size(0), mini_batch_size):            \n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))            \n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size).squeeze())\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc_loss_vector[e] = acc_loss\n",
    "        print('epoch: ' +  str(e) + ', loss: ' + str(acc_loss) )\n",
    "    \n",
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):    \n",
    "    nb_data_errors = 0    \n",
    "    with torch.no_grad():\n",
    "        for b in range(0, data_input.size(0), mini_batch_size):\n",
    "            output = model(data_input.narrow(0, b, mini_batch_size))    \n",
    "            _, predicted = torch.max(output, 1)    \n",
    "            for k in range(mini_batch_size):\n",
    "                if data_target[b + k].view(-1, 3).max(1)[1]  != predicted[k] :\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "    return nb_data_errors            \n",
    "\n",
    "#######################################################################\n",
    "mini_batch_size = 1000\n",
    "nb_hidden = 200\n",
    "\n",
    "# to train in GPU if available \n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cuda')\n",
    "# print('working device ', device)\n",
    "# Net2.to(device)\n",
    "# x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "# # train with H states only\n",
    "# label_class_3_train = y_train == torch.tensor([0., 0., 1.]).view(-1,3) \n",
    "# label_class_3_train = label_class_3_train + 0.0\n",
    "# mask_H = label_class_3_train.prod(dim = 1) > 0 \n",
    "\n",
    "# # data to feed model\n",
    "# data_train_H = x_train[mask_H] [: 20000]\n",
    "# labels_train_H = y_train[mask_H] [: 20000]\n",
    "\n",
    "# model training\n",
    "nb_epochs = 10\n",
    "acc_loss_vector = torch.zeros(nb_epochs) \n",
    "model = Net(nb_hidden)\n",
    "model.train()\n",
    "print('training started')\n",
    "train_model(model, x_train, y_train, mini_batch_size, acc_loss_vector)\n",
    "print('training finished')\n",
    "\n",
    "# data to test model accuracy\n",
    "label_class_2_test = y_test == torch.tensor([0., 1., 0.]).view(-1,3) \n",
    "label_class_2_test = label_class_2_test + 0.0\n",
    "mask_H_test = label_class_2_test.prod(dim = 1) > 0 \n",
    "x_test_H = x_test[mask_H_test][: 3000]\n",
    "y_test_H = y_test[mask_H_test][: 3000]\n",
    "\n",
    "# model(data_test_H)\n",
    "#%%\n",
    "#######################################################################\n",
    "nb_train_errors = compute_nb_errors(model, x_train, y_train, mini_batch_size) / x_train.size(0) * 100\n",
    "print('train error: ', str(nb_train_errors) + '%')\n",
    "\n",
    "nb_test_errors = compute_nb_errors(model, x_test, y_test, mini_batch_size) / x_test.size(0) * 100\n",
    "print('test error: ' +str(nb_test_errors) + '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "36004ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127832, 20, 4)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
